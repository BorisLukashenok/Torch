{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домшнее задание 3\n",
    "\n",
    "## Задача:  \n",
    "\n",
    "- Создать Dataset для загрузки данных (sklearn.datasets.fetch_california_housing)  \n",
    "- Обернуть его в Dataloader  \n",
    "- Написать архитектуру сети, которая предсказывает стоимость недвижимости. Сеть должна включать BatchNorm слои и Dropout (или НЕ включать, но нужно обосновать)  \n",
    "- Сравните сходимость Adam, RMSProp и SGD, сделайте вывод по качеству работы модели train-test разделение нужно сделать с помощью sklearn random_state=13, test_size = 0.25  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn import MSELoss, Linear, ReLU, Dropout, BatchNorm1d, Sequential, Sigmoid\n",
    "from torch.optim import SGD, Adam, RMSprop\n",
    "from torchsummary import summary\n",
    "from tqdm import trange\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 1. Создать Dataset для загрузки данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "houses = fetch_california_housing()\n",
    "x, y = houses.data, houses.target\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=13, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_StandardScaler(x):\n",
    "    m = x.mean(0, keepdim=True)\n",
    "    s = x.std(0, unbiased=False, keepdim=True)\n",
    "    x -= m\n",
    "    x /= s\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch_StandardScaler(torch.FloatTensor(x_train))\n",
    "y_train = torch.FloatTensor(y_train).view(-1, 1)\n",
    "\n",
    "x_test = torch_StandardScaler(torch.FloatTensor(x_test))\n",
    "y_test = torch.FloatTensor(y_test).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(x_train, y_train)\n",
    "test_data = TensorDataset(x_test,  y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Обернуть его в Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242 81\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=64)\n",
    "print(len(train_loader), len(test_loader))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Написать архитектуру сети, которая предсказывает стоимость недвижимости."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def great_california_model(input_count, output_count, layers_count=3, multiplier=4, batch=True, drop=True):\n",
    "        \n",
    "    hidden_layers_coef = list(range(layers_count * multiplier, 0, -multiplier) )\n",
    "    hidden_layers_coef.append(output_count)\n",
    "    \n",
    "    in_f = 0\n",
    "    out_f = 0\n",
    "    layer_list = []\n",
    "    for layer_i in range(layers_count):\n",
    "        match layer_i:\n",
    "            case 0:\n",
    "                in_f = input_count                \n",
    "            case _ if layer_i > 0:\n",
    "                in_f = hidden_layers_coef[layer_i] * input_count\n",
    "        match layer_i:\n",
    "            case _ if layer_i != layers_count - 1:\n",
    "                out_f = hidden_layers_coef[layer_i + 1] * input_count\n",
    "            case _:\n",
    "                out_f = output_count\n",
    "\n",
    "        layer_list.append(Linear(in_f, out_f))\n",
    "        \n",
    "        if layer_i != layers_count - 1:            \n",
    "            if batch: layer_list.append(BatchNorm1d(out_f))\n",
    "            layer_list.append(ReLU())\n",
    "            if drop: layer_list.append(Dropout(0.25))  \n",
    "    return Sequential(*layer_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CaliforniaNet = great_california_model(8,1).cuda()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                   [-1, 64]             576\n",
      "       BatchNorm1d-2                   [-1, 64]             128\n",
      "              ReLU-3                   [-1, 64]               0\n",
      "           Dropout-4                   [-1, 64]               0\n",
      "            Linear-5                   [-1, 32]           2,080\n",
      "       BatchNorm1d-6                   [-1, 32]              64\n",
      "              ReLU-7                   [-1, 32]               0\n",
      "           Dropout-8                   [-1, 32]               0\n",
      "            Linear-9                    [-1, 1]              33\n",
      "================================================================\n",
      "Total params: 2,881\n",
      "Trainable params: 2,881\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.01\n",
      "Estimated Total Size (MB): 0.01\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(CaliforniaNet, (8,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=8, out_features=64, bias=True)\n",
       "  (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU()\n",
       "  (3): Dropout(p=0.25, inplace=False)\n",
       "  (4): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (5): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (6): ReLU()\n",
       "  (7): Dropout(p=0.25, inplace=False)\n",
       "  (8): Linear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CaliforniaNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Сравните сходимость Adam, RMSProp и SGD, сделайте вывод по качеству работы модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_list = [SGD, Adam, RMSprop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 001/100: 100%|██████████| 242/242 [00:00<00:00, 382.52it/s, optimizer=>SGD, loss=0.775]\n",
      "Epoch 002/100: 100%|██████████| 242/242 [00:00<00:00, 367.43it/s, optimizer=>SGD, loss=0.548]\n",
      "Epoch 003/100: 100%|██████████| 242/242 [00:00<00:00, 383.13it/s, optimizer=>SGD, loss=0.496]\n",
      "Epoch 004/100: 100%|██████████| 242/242 [00:00<00:00, 401.54it/s, optimizer=>SGD, loss=0.502]\n",
      "Epoch 005/100: 100%|██████████| 242/242 [00:00<00:00, 407.64it/s, optimizer=>SGD, loss=0.473]\n",
      "Epoch 006/100: 100%|██████████| 242/242 [00:00<00:00, 381.32it/s, optimizer=>SGD, loss=0.475]\n",
      "Epoch 007/100: 100%|██████████| 242/242 [00:00<00:00, 364.67it/s, optimizer=>SGD, loss=0.468]\n",
      "Epoch 008/100: 100%|██████████| 242/242 [00:00<00:00, 395.01it/s, optimizer=>SGD, loss=0.455]\n",
      "Epoch 009/100: 100%|██████████| 242/242 [00:00<00:00, 359.26it/s, optimizer=>SGD, loss=0.455]\n",
      "Epoch 010/100: 100%|██████████| 242/242 [00:00<00:00, 321.57it/s, optimizer=>SGD, loss=0.449]\n",
      "Epoch 011/100: 100%|██████████| 242/242 [00:00<00:00, 390.55it/s, optimizer=>SGD, loss=0.434]\n",
      "Epoch 012/100: 100%|██████████| 242/242 [00:00<00:00, 388.04it/s, optimizer=>SGD, loss=0.444]\n",
      "Epoch 013/100: 100%|██████████| 242/242 [00:00<00:00, 334.45it/s, optimizer=>SGD, loss=0.430]\n",
      "Epoch 014/100: 100%|██████████| 242/242 [00:00<00:00, 305.34it/s, optimizer=>SGD, loss=0.440]\n",
      "Epoch 015/100: 100%|██████████| 242/242 [00:00<00:00, 349.41it/s, optimizer=>SGD, loss=0.437]\n",
      "Epoch 016/100: 100%|██████████| 242/242 [00:00<00:00, 350.93it/s, optimizer=>SGD, loss=0.430]\n",
      "Epoch 017/100: 100%|██████████| 242/242 [00:00<00:00, 355.56it/s, optimizer=>SGD, loss=0.428]\n",
      "Epoch 018/100: 100%|██████████| 242/242 [00:00<00:00, 339.61it/s, optimizer=>SGD, loss=0.431]\n",
      "Epoch 019/100: 100%|██████████| 242/242 [00:00<00:00, 319.87it/s, optimizer=>SGD, loss=0.429]\n",
      "Epoch 020/100: 100%|██████████| 242/242 [00:00<00:00, 344.92it/s, optimizer=>SGD, loss=0.427]\n",
      "Epoch 021/100: 100%|██████████| 242/242 [00:00<00:00, 344.92it/s, optimizer=>SGD, loss=0.422]\n",
      "Epoch 022/100: 100%|██████████| 242/242 [00:00<00:00, 341.04it/s, optimizer=>SGD, loss=0.424]\n",
      "Epoch 023/100: 100%|██████████| 242/242 [00:00<00:00, 343.95it/s, optimizer=>SGD, loss=0.423]\n",
      "Epoch 024/100: 100%|██████████| 242/242 [00:00<00:00, 337.24it/s, optimizer=>SGD, loss=0.416]\n",
      "Epoch 025/100: 100%|██████████| 242/242 [00:00<00:00, 340.56it/s, optimizer=>SGD, loss=0.415]\n",
      "Epoch 026/100: 100%|██████████| 242/242 [00:00<00:00, 320.29it/s, optimizer=>SGD, loss=0.413]\n",
      "Epoch 027/100: 100%|██████████| 242/242 [00:00<00:00, 336.46it/s, optimizer=>SGD, loss=0.410]\n",
      "Epoch 028/100: 100%|██████████| 242/242 [00:00<00:00, 340.55it/s, optimizer=>SGD, loss=0.419]\n",
      "Epoch 029/100: 100%|██████████| 242/242 [00:00<00:00, 335.36it/s, optimizer=>SGD, loss=0.410]\n",
      "Epoch 030/100: 100%|██████████| 242/242 [00:00<00:00, 332.15it/s, optimizer=>SGD, loss=0.407]\n",
      "Epoch 031/100: 100%|██████████| 242/242 [00:00<00:00, 307.28it/s, optimizer=>SGD, loss=0.409]\n",
      "Epoch 032/100: 100%|██████████| 242/242 [00:00<00:00, 342.97it/s, optimizer=>SGD, loss=0.408]\n",
      "Epoch 033/100: 100%|██████████| 242/242 [00:00<00:00, 359.78it/s, optimizer=>SGD, loss=0.399]\n",
      "Epoch 034/100: 100%|██████████| 242/242 [00:00<00:00, 344.44it/s, optimizer=>SGD, loss=0.411]\n",
      "Epoch 035/100: 100%|██████████| 242/242 [00:00<00:00, 326.33it/s, optimizer=>SGD, loss=0.407]\n",
      "Epoch 036/100: 100%|██████████| 242/242 [00:00<00:00, 338.18it/s, optimizer=>SGD, loss=0.401]\n",
      "Epoch 037/100: 100%|██████████| 242/242 [00:00<00:00, 277.36it/s, optimizer=>SGD, loss=0.403]\n",
      "Epoch 038/100: 100%|██████████| 242/242 [00:00<00:00, 331.24it/s, optimizer=>SGD, loss=0.416]\n",
      "Epoch 039/100: 100%|██████████| 242/242 [00:00<00:00, 271.15it/s, optimizer=>SGD, loss=0.403]\n",
      "Epoch 040/100: 100%|██████████| 242/242 [00:00<00:00, 328.99it/s, optimizer=>SGD, loss=0.400]\n",
      "Epoch 041/100: 100%|██████████| 242/242 [00:00<00:00, 330.33it/s, optimizer=>SGD, loss=0.406]\n",
      "Epoch 042/100: 100%|██████████| 242/242 [00:00<00:00, 344.45it/s, optimizer=>SGD, loss=0.400]\n",
      "Epoch 043/100: 100%|██████████| 242/242 [00:00<00:00, 346.40it/s, optimizer=>SGD, loss=0.399]\n",
      "Epoch 044/100: 100%|██████████| 242/242 [00:00<00:00, 327.65it/s, optimizer=>SGD, loss=0.405]\n",
      "Epoch 045/100: 100%|██████████| 242/242 [00:00<00:00, 338.66it/s, optimizer=>SGD, loss=0.415]\n",
      "Epoch 046/100: 100%|██████████| 242/242 [00:00<00:00, 343.45it/s, optimizer=>SGD, loss=0.396]\n",
      "Epoch 047/100: 100%|██████████| 242/242 [00:00<00:00, 344.92it/s, optimizer=>SGD, loss=0.395]\n",
      "Epoch 048/100: 100%|██████████| 242/242 [00:00<00:00, 343.45it/s, optimizer=>SGD, loss=0.403]\n",
      "Epoch 049/100: 100%|██████████| 242/242 [00:00<00:00, 344.93it/s, optimizer=>SGD, loss=0.403]\n",
      "Epoch 050/100: 100%|██████████| 242/242 [00:00<00:00, 341.52it/s, optimizer=>SGD, loss=0.395]\n",
      "Epoch 051/100: 100%|██████████| 242/242 [00:00<00:00, 317.35it/s, optimizer=>SGD, loss=0.407]\n",
      "Epoch 052/100: 100%|██████████| 242/242 [00:00<00:00, 339.60it/s, optimizer=>SGD, loss=0.397]\n",
      "Epoch 053/100: 100%|██████████| 242/242 [00:00<00:00, 342.97it/s, optimizer=>SGD, loss=0.402]\n",
      "Epoch 054/100: 100%|██████████| 242/242 [00:00<00:00, 342.00it/s, optimizer=>SGD, loss=0.416]\n",
      "Epoch 055/100: 100%|██████████| 242/242 [00:00<00:00, 344.93it/s, optimizer=>SGD, loss=0.397]\n",
      "Epoch 056/100: 100%|██████████| 242/242 [00:00<00:00, 342.48it/s, optimizer=>SGD, loss=0.398]\n",
      "Epoch 057/100: 100%|██████████| 242/242 [00:00<00:00, 342.49it/s, optimizer=>SGD, loss=0.401]\n",
      "Epoch 058/100: 100%|██████████| 242/242 [00:00<00:00, 306.50it/s, optimizer=>SGD, loss=0.400]\n",
      "Epoch 059/100: 100%|██████████| 242/242 [00:00<00:00, 328.53it/s, optimizer=>SGD, loss=0.399]\n",
      "Epoch 060/100: 100%|██████████| 242/242 [00:00<00:00, 342.97it/s, optimizer=>SGD, loss=0.395]\n",
      "Epoch 061/100: 100%|██████████| 242/242 [00:00<00:00, 342.48it/s, optimizer=>SGD, loss=0.394]\n",
      "Epoch 062/100: 100%|██████████| 242/242 [00:00<00:00, 341.52it/s, optimizer=>SGD, loss=0.395]\n",
      "Epoch 063/100: 100%|██████████| 242/242 [00:00<00:00, 341.52it/s, optimizer=>SGD, loss=0.400]\n",
      "Epoch 064/100: 100%|██████████| 242/242 [00:00<00:00, 314.47it/s, optimizer=>SGD, loss=0.408]\n",
      "Epoch 065/100: 100%|██████████| 242/242 [00:00<00:00, 341.03it/s, optimizer=>SGD, loss=0.396]\n",
      "Epoch 066/100: 100%|██████████| 242/242 [00:00<00:00, 344.44it/s, optimizer=>SGD, loss=0.391]\n",
      "Epoch 067/100: 100%|██████████| 242/242 [00:00<00:00, 323.28it/s, optimizer=>SGD, loss=0.393]\n",
      "Epoch 068/100: 100%|██████████| 242/242 [00:00<00:00, 338.66it/s, optimizer=>SGD, loss=0.398]\n",
      "Epoch 069/100: 100%|██████████| 242/242 [00:00<00:00, 340.56it/s, optimizer=>SGD, loss=0.395]\n",
      "Epoch 070/100: 100%|██████████| 242/242 [00:00<00:00, 336.77it/s, optimizer=>SGD, loss=0.394]\n",
      "Epoch 071/100: 100%|██████████| 242/242 [00:00<00:00, 319.02it/s, optimizer=>SGD, loss=0.390]\n",
      "Epoch 072/100: 100%|██████████| 242/242 [00:00<00:00, 339.12it/s, optimizer=>SGD, loss=0.384]\n",
      "Epoch 073/100: 100%|██████████| 242/242 [00:00<00:00, 339.12it/s, optimizer=>SGD, loss=0.396]\n",
      "Epoch 074/100: 100%|██████████| 242/242 [00:00<00:00, 343.46it/s, optimizer=>SGD, loss=0.394]\n",
      "Epoch 075/100: 100%|██████████| 242/242 [00:00<00:00, 347.41it/s, optimizer=>SGD, loss=0.395]\n",
      "Epoch 076/100: 100%|██████████| 242/242 [00:00<00:00, 342.97it/s, optimizer=>SGD, loss=0.384]\n",
      "Epoch 077/100: 100%|██████████| 242/242 [00:00<00:00, 325.89it/s, optimizer=>SGD, loss=0.396]\n",
      "Epoch 078/100: 100%|██████████| 242/242 [00:00<00:00, 339.13it/s, optimizer=>SGD, loss=0.385]\n",
      "Epoch 079/100: 100%|██████████| 242/242 [00:00<00:00, 336.77it/s, optimizer=>SGD, loss=0.394]\n",
      "Epoch 080/100: 100%|██████████| 242/242 [00:00<00:00, 340.08it/s, optimizer=>SGD, loss=0.392]\n",
      "Epoch 081/100: 100%|██████████| 242/242 [00:00<00:00, 308.46it/s, optimizer=>SGD, loss=0.385]\n",
      "Epoch 082/100: 100%|██████████| 242/242 [00:00<00:00, 342.00it/s, optimizer=>SGD, loss=0.388]\n",
      "Epoch 083/100: 100%|██████████| 242/242 [00:00<00:00, 325.89it/s, optimizer=>SGD, loss=0.393]\n",
      "Epoch 084/100: 100%|██████████| 242/242 [00:00<00:00, 342.00it/s, optimizer=>SGD, loss=0.388]\n",
      "Epoch 085/100: 100%|██████████| 242/242 [00:00<00:00, 340.55it/s, optimizer=>SGD, loss=0.388]\n",
      "Epoch 086/100: 100%|██████████| 242/242 [00:00<00:00, 338.18it/s, optimizer=>SGD, loss=0.391]\n",
      "Epoch 087/100: 100%|██████████| 242/242 [00:00<00:00, 344.44it/s, optimizer=>SGD, loss=0.390]\n",
      "Epoch 088/100: 100%|██████████| 242/242 [00:00<00:00, 342.00it/s, optimizer=>SGD, loss=0.387]\n",
      "Epoch 089/100: 100%|██████████| 242/242 [00:00<00:00, 322.42it/s, optimizer=>SGD, loss=0.380]\n",
      "Epoch 090/100: 100%|██████████| 242/242 [00:00<00:00, 338.66it/s, optimizer=>SGD, loss=0.391]\n",
      "Epoch 091/100: 100%|██████████| 242/242 [00:00<00:00, 344.43it/s, optimizer=>SGD, loss=0.393]\n",
      "Epoch 092/100: 100%|██████████| 242/242 [00:00<00:00, 341.52it/s, optimizer=>SGD, loss=0.385]\n",
      "Epoch 093/100: 100%|██████████| 242/242 [00:00<00:00, 342.97it/s, optimizer=>SGD, loss=0.389]\n",
      "Epoch 094/100: 100%|██████████| 242/242 [00:00<00:00, 342.01it/s, optimizer=>SGD, loss=0.394]\n",
      "Epoch 095/100: 100%|██████████| 242/242 [00:00<00:00, 347.39it/s, optimizer=>SGD, loss=0.393]\n",
      "Epoch 096/100: 100%|██████████| 242/242 [00:00<00:00, 319.02it/s, optimizer=>SGD, loss=0.378]\n",
      "Epoch 097/100: 100%|██████████| 242/242 [00:00<00:00, 341.03it/s, optimizer=>SGD, loss=0.384]\n",
      "Epoch 098/100: 100%|██████████| 242/242 [00:00<00:00, 340.55it/s, optimizer=>SGD, loss=0.401]\n",
      "Epoch 099/100: 100%|██████████| 242/242 [00:00<00:00, 342.48it/s, optimizer=>SGD, loss=0.377]\n",
      "Epoch 100/100: 100%|██████████| 242/242 [00:00<00:00, 341.52it/s, optimizer=>SGD, loss=0.383]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 001/100: 100%|██████████| 242/242 [00:00<00:00, 290.68it/s, optimizer=>Adam, loss=1.296]\n",
      "Epoch 002/100: 100%|██████████| 242/242 [00:00<00:00, 311.23it/s, optimizer=>Adam, loss=0.655]\n",
      "Epoch 003/100: 100%|██████████| 242/242 [00:00<00:00, 303.43it/s, optimizer=>Adam, loss=0.574]\n",
      "Epoch 004/100: 100%|██████████| 242/242 [00:00<00:00, 274.22it/s, optimizer=>Adam, loss=0.543]\n",
      "Epoch 005/100: 100%|██████████| 242/242 [00:00<00:00, 311.63it/s, optimizer=>Adam, loss=0.521]\n",
      "Epoch 006/100: 100%|██████████| 242/242 [00:00<00:00, 307.28it/s, optimizer=>Adam, loss=0.506]\n",
      "Epoch 007/100: 100%|██████████| 242/242 [00:00<00:00, 273.29it/s, optimizer=>Adam, loss=0.495]\n",
      "Epoch 008/100: 100%|██████████| 242/242 [00:00<00:00, 247.59it/s, optimizer=>Adam, loss=0.474]\n",
      "Epoch 009/100: 100%|██████████| 242/242 [00:00<00:00, 326.33it/s, optimizer=>Adam, loss=0.452]\n",
      "Epoch 010/100: 100%|██████████| 242/242 [00:00<00:00, 335.84it/s, optimizer=>Adam, loss=0.459]\n",
      "Epoch 011/100: 100%|██████████| 242/242 [00:00<00:00, 302.30it/s, optimizer=>Adam, loss=0.452]\n",
      "Epoch 012/100: 100%|██████████| 242/242 [00:00<00:00, 276.41it/s, optimizer=>Adam, loss=0.445]\n",
      "Epoch 013/100: 100%|██████████| 242/242 [00:00<00:00, 328.10it/s, optimizer=>Adam, loss=0.435]\n",
      "Epoch 014/100: 100%|██████████| 242/242 [00:00<00:00, 294.57it/s, optimizer=>Adam, loss=0.442]\n",
      "Epoch 015/100: 100%|██████████| 242/242 [00:00<00:00, 319.44it/s, optimizer=>Adam, loss=0.426]\n",
      "Epoch 016/100: 100%|██████████| 242/242 [00:00<00:00, 309.64it/s, optimizer=>Adam, loss=0.418]\n",
      "Epoch 017/100: 100%|██████████| 242/242 [00:00<00:00, 303.43it/s, optimizer=>Adam, loss=0.425]\n",
      "Epoch 018/100: 100%|██████████| 242/242 [00:00<00:00, 307.28it/s, optimizer=>Adam, loss=0.421]\n",
      "Epoch 019/100: 100%|██████████| 242/242 [00:00<00:00, 309.64it/s, optimizer=>Adam, loss=0.424]\n",
      "Epoch 020/100: 100%|██████████| 242/242 [00:00<00:00, 309.63it/s, optimizer=>Adam, loss=0.410]\n",
      "Epoch 021/100: 100%|██████████| 242/242 [00:00<00:00, 286.22it/s, optimizer=>Adam, loss=0.418]\n",
      "Epoch 022/100: 100%|██████████| 242/242 [00:00<00:00, 304.19it/s, optimizer=>Adam, loss=0.414]\n",
      "Epoch 023/100: 100%|██████████| 242/242 [00:00<00:00, 301.54it/s, optimizer=>Adam, loss=0.412]\n",
      "Epoch 024/100: 100%|██████████| 242/242 [00:00<00:00, 316.52it/s, optimizer=>Adam, loss=0.408]\n",
      "Epoch 025/100: 100%|██████████| 242/242 [00:00<00:00, 304.19it/s, optimizer=>Adam, loss=0.411]\n",
      "Epoch 026/100: 100%|██████████| 242/242 [00:00<00:00, 314.05it/s, optimizer=>Adam, loss=0.408]\n",
      "Epoch 027/100: 100%|██████████| 242/242 [00:00<00:00, 294.93it/s, optimizer=>Adam, loss=0.399]\n",
      "Epoch 028/100: 100%|██████████| 242/242 [00:00<00:00, 314.87it/s, optimizer=>Adam, loss=0.406]\n",
      "Epoch 029/100: 100%|██████████| 242/242 [00:00<00:00, 301.17it/s, optimizer=>Adam, loss=0.408]\n",
      "Epoch 030/100: 100%|██████████| 242/242 [00:00<00:00, 278.63it/s, optimizer=>Adam, loss=0.406]\n",
      "Epoch 031/100: 100%|██████████| 242/242 [00:00<00:00, 326.76it/s, optimizer=>Adam, loss=0.399]\n",
      "Epoch 032/100: 100%|██████████| 242/242 [00:00<00:00, 339.61it/s, optimizer=>Adam, loss=0.400]\n",
      "Epoch 033/100: 100%|██████████| 242/242 [00:00<00:00, 278.96it/s, optimizer=>Adam, loss=0.405]\n",
      "Epoch 034/100: 100%|██████████| 242/242 [00:00<00:00, 288.26it/s, optimizer=>Adam, loss=0.399]\n",
      "Epoch 035/100: 100%|██████████| 242/242 [00:00<00:00, 308.18it/s, optimizer=>Adam, loss=0.407]\n",
      "Epoch 036/100: 100%|██████████| 242/242 [00:00<00:00, 275.47it/s, optimizer=>Adam, loss=0.405]\n",
      "Epoch 037/100: 100%|██████████| 242/242 [00:00<00:00, 298.57it/s, optimizer=>Adam, loss=0.395]\n",
      "Epoch 038/100: 100%|██████████| 242/242 [00:00<00:00, 296.01it/s, optimizer=>Adam, loss=0.392]\n",
      "Epoch 039/100: 100%|██████████| 242/242 [00:00<00:00, 296.38it/s, optimizer=>Adam, loss=0.400]\n",
      "Epoch 040/100: 100%|██████████| 242/242 [00:00<00:00, 310.83it/s, optimizer=>Adam, loss=0.397]\n",
      "Epoch 041/100: 100%|██████████| 242/242 [00:00<00:00, 280.25it/s, optimizer=>Adam, loss=0.398]\n",
      "Epoch 042/100: 100%|██████████| 242/242 [00:00<00:00, 315.28it/s, optimizer=>Adam, loss=0.399]\n",
      "Epoch 043/100: 100%|██████████| 242/242 [00:00<00:00, 328.10it/s, optimizer=>Adam, loss=0.386]\n",
      "Epoch 044/100: 100%|██████████| 242/242 [00:00<00:00, 312.44it/s, optimizer=>Adam, loss=0.399]\n",
      "Epoch 045/100: 100%|██████████| 242/242 [00:00<00:00, 294.93it/s, optimizer=>Adam, loss=0.396]\n",
      "Epoch 046/100: 100%|██████████| 242/242 [00:00<00:00, 314.87it/s, optimizer=>Adam, loss=0.396]\n",
      "Epoch 047/100: 100%|██████████| 242/242 [00:00<00:00, 315.28it/s, optimizer=>Adam, loss=0.395]\n",
      "Epoch 048/100: 100%|██████████| 242/242 [00:00<00:00, 275.78it/s, optimizer=>Adam, loss=0.385]\n",
      "Epoch 049/100: 100%|██████████| 242/242 [00:00<00:00, 313.65it/s, optimizer=>Adam, loss=0.393]\n",
      "Epoch 050/100: 100%|██████████| 242/242 [00:00<00:00, 288.60it/s, optimizer=>Adam, loss=0.388]\n",
      "Epoch 051/100: 100%|██████████| 242/242 [00:00<00:00, 261.48it/s, optimizer=>Adam, loss=0.389]\n",
      "Epoch 052/100: 100%|██████████| 242/242 [00:00<00:00, 315.88it/s, optimizer=>Adam, loss=0.386]\n",
      "Epoch 053/100: 100%|██████████| 242/242 [00:00<00:00, 323.28it/s, optimizer=>Adam, loss=0.384]\n",
      "Epoch 054/100: 100%|██████████| 242/242 [00:00<00:00, 310.43it/s, optimizer=>Adam, loss=0.387]\n",
      "Epoch 055/100: 100%|██████████| 242/242 [00:00<00:00, 304.30it/s, optimizer=>Adam, loss=0.381]\n",
      "Epoch 056/100: 100%|██████████| 242/242 [00:00<00:00, 320.29it/s, optimizer=>Adam, loss=0.381]\n",
      "Epoch 057/100: 100%|██████████| 242/242 [00:00<00:00, 290.06it/s, optimizer=>Adam, loss=0.389]\n",
      "Epoch 058/100: 100%|██████████| 242/242 [00:00<00:00, 314.63it/s, optimizer=>Adam, loss=0.385]\n",
      "Epoch 059/100: 100%|██████████| 242/242 [00:00<00:00, 323.71it/s, optimizer=>Adam, loss=0.389]\n",
      "Epoch 060/100: 100%|██████████| 242/242 [00:00<00:00, 295.01it/s, optimizer=>Adam, loss=0.387]\n",
      "Epoch 061/100: 100%|██████████| 242/242 [00:00<00:00, 318.18it/s, optimizer=>Adam, loss=0.384]\n",
      "Epoch 062/100: 100%|██████████| 242/242 [00:00<00:00, 320.24it/s, optimizer=>Adam, loss=0.384]\n",
      "Epoch 063/100: 100%|██████████| 242/242 [00:00<00:00, 279.05it/s, optimizer=>Adam, loss=0.378]\n",
      "Epoch 064/100: 100%|██████████| 242/242 [00:00<00:00, 323.91it/s, optimizer=>Adam, loss=0.381]\n",
      "Epoch 065/100: 100%|██████████| 242/242 [00:00<00:00, 303.36it/s, optimizer=>Adam, loss=0.382]\n",
      "Epoch 066/100: 100%|██████████| 242/242 [00:00<00:00, 309.53it/s, optimizer=>Adam, loss=0.389]\n",
      "Epoch 067/100: 100%|██████████| 242/242 [00:00<00:00, 292.63it/s, optimizer=>Adam, loss=0.378]\n",
      "Epoch 068/100: 100%|██████████| 242/242 [00:00<00:00, 269.33it/s, optimizer=>Adam, loss=0.389]\n",
      "Epoch 069/100: 100%|██████████| 242/242 [00:00<00:00, 320.47it/s, optimizer=>Adam, loss=0.383]\n",
      "Epoch 070/100: 100%|██████████| 242/242 [00:00<00:00, 316.23it/s, optimizer=>Adam, loss=0.381]\n",
      "Epoch 071/100: 100%|██████████| 242/242 [00:00<00:00, 304.21it/s, optimizer=>Adam, loss=0.377]\n",
      "Epoch 072/100: 100%|██████████| 242/242 [00:00<00:00, 316.50it/s, optimizer=>Adam, loss=0.378]\n",
      "Epoch 073/100: 100%|██████████| 242/242 [00:00<00:00, 296.01it/s, optimizer=>Adam, loss=0.379]\n",
      "Epoch 074/100: 100%|██████████| 242/242 [00:00<00:00, 296.81it/s, optimizer=>Adam, loss=0.374]\n",
      "Epoch 075/100: 100%|██████████| 242/242 [00:00<00:00, 317.11it/s, optimizer=>Adam, loss=0.391]\n",
      "Epoch 076/100: 100%|██████████| 242/242 [00:00<00:00, 298.20it/s, optimizer=>Adam, loss=0.386]\n",
      "Epoch 077/100: 100%|██████████| 242/242 [00:00<00:00, 318.02it/s, optimizer=>Adam, loss=0.376]\n",
      "Epoch 078/100: 100%|██████████| 242/242 [00:00<00:00, 299.76it/s, optimizer=>Adam, loss=0.383]\n",
      "Epoch 079/100: 100%|██████████| 242/242 [00:00<00:00, 319.72it/s, optimizer=>Adam, loss=0.381]\n",
      "Epoch 080/100: 100%|██████████| 242/242 [00:00<00:00, 300.36it/s, optimizer=>Adam, loss=0.386]\n",
      "Epoch 081/100: 100%|██████████| 242/242 [00:00<00:00, 290.09it/s, optimizer=>Adam, loss=0.383]\n",
      "Epoch 082/100: 100%|██████████| 242/242 [00:00<00:00, 321.19it/s, optimizer=>Adam, loss=0.378]\n",
      "Epoch 083/100: 100%|██████████| 242/242 [00:00<00:00, 311.63it/s, optimizer=>Adam, loss=0.382]\n",
      "Epoch 084/100: 100%|██████████| 242/242 [00:00<00:00, 306.57it/s, optimizer=>Adam, loss=0.383]\n",
      "Epoch 085/100: 100%|██████████| 242/242 [00:00<00:00, 316.93it/s, optimizer=>Adam, loss=0.373]\n",
      "Epoch 086/100: 100%|██████████| 242/242 [00:00<00:00, 247.96it/s, optimizer=>Adam, loss=0.381]\n",
      "Epoch 087/100: 100%|██████████| 242/242 [00:00<00:00, 322.95it/s, optimizer=>Adam, loss=0.379]\n",
      "Epoch 088/100: 100%|██████████| 242/242 [00:00<00:00, 315.41it/s, optimizer=>Adam, loss=0.386]\n",
      "Epoch 089/100: 100%|██████████| 242/242 [00:00<00:00, 292.79it/s, optimizer=>Adam, loss=0.379]\n",
      "Epoch 090/100: 100%|██████████| 242/242 [00:00<00:00, 318.69it/s, optimizer=>Adam, loss=0.375]\n",
      "Epoch 091/100: 100%|██████████| 242/242 [00:00<00:00, 319.43it/s, optimizer=>Adam, loss=0.382]\n",
      "Epoch 092/100: 100%|██████████| 242/242 [00:00<00:00, 279.44it/s, optimizer=>Adam, loss=0.377]\n",
      "Epoch 093/100: 100%|██████████| 242/242 [00:00<00:00, 316.24it/s, optimizer=>Adam, loss=0.381]\n",
      "Epoch 094/100: 100%|██████████| 242/242 [00:00<00:00, 309.67it/s, optimizer=>Adam, loss=0.384]\n",
      "Epoch 095/100: 100%|██████████| 242/242 [00:00<00:00, 293.53it/s, optimizer=>Adam, loss=0.378]\n",
      "Epoch 096/100: 100%|██████████| 242/242 [00:00<00:00, 323.15it/s, optimizer=>Adam, loss=0.374]\n",
      "Epoch 097/100: 100%|██████████| 242/242 [00:00<00:00, 290.46it/s, optimizer=>Adam, loss=0.386]\n",
      "Epoch 098/100: 100%|██████████| 242/242 [00:00<00:00, 310.00it/s, optimizer=>Adam, loss=0.372]\n",
      "Epoch 099/100: 100%|██████████| 242/242 [00:00<00:00, 316.52it/s, optimizer=>Adam, loss=0.374]\n",
      "Epoch 100/100: 100%|██████████| 242/242 [00:00<00:00, 303.18it/s, optimizer=>Adam, loss=0.384]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSprop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 001/100: 100%|██████████| 242/242 [00:00<00:00, 342.97it/s, optimizer=>RMSprop, loss=0.645]\n",
      "Epoch 002/100: 100%|██████████| 242/242 [00:00<00:00, 339.86it/s, optimizer=>RMSprop, loss=0.476]\n",
      "Epoch 003/100: 100%|██████████| 242/242 [00:00<00:00, 310.14it/s, optimizer=>RMSprop, loss=0.466]\n",
      "Epoch 004/100: 100%|██████████| 242/242 [00:00<00:00, 342.47it/s, optimizer=>RMSprop, loss=0.443]\n",
      "Epoch 005/100: 100%|██████████| 242/242 [00:00<00:00, 340.14it/s, optimizer=>RMSprop, loss=0.430]\n",
      "Epoch 006/100: 100%|██████████| 242/242 [00:00<00:00, 330.91it/s, optimizer=>RMSprop, loss=0.424]\n",
      "Epoch 007/100: 100%|██████████| 242/242 [00:00<00:00, 344.93it/s, optimizer=>RMSprop, loss=0.424]\n",
      "Epoch 008/100: 100%|██████████| 242/242 [00:00<00:00, 326.21it/s, optimizer=>RMSprop, loss=0.421]\n",
      "Epoch 009/100: 100%|██████████| 242/242 [00:00<00:00, 345.20it/s, optimizer=>RMSprop, loss=0.418]\n",
      "Epoch 010/100: 100%|██████████| 242/242 [00:00<00:00, 324.11it/s, optimizer=>RMSprop, loss=0.427]\n",
      "Epoch 011/100: 100%|██████████| 242/242 [00:00<00:00, 326.82it/s, optimizer=>RMSprop, loss=0.409]\n",
      "Epoch 012/100: 100%|██████████| 242/242 [00:00<00:00, 349.89it/s, optimizer=>RMSprop, loss=0.411]\n",
      "Epoch 013/100: 100%|██████████| 242/242 [00:00<00:00, 312.37it/s, optimizer=>RMSprop, loss=0.416]\n",
      "Epoch 014/100: 100%|██████████| 242/242 [00:00<00:00, 357.80it/s, optimizer=>RMSprop, loss=0.407]\n",
      "Epoch 015/100: 100%|██████████| 242/242 [00:00<00:00, 381.37it/s, optimizer=>RMSprop, loss=0.412]\n",
      "Epoch 016/100: 100%|██████████| 242/242 [00:00<00:00, 378.34it/s, optimizer=>RMSprop, loss=0.408]\n",
      "Epoch 017/100: 100%|██████████| 242/242 [00:00<00:00, 349.12it/s, optimizer=>RMSprop, loss=0.407]\n",
      "Epoch 018/100: 100%|██████████| 242/242 [00:00<00:00, 374.92it/s, optimizer=>RMSprop, loss=0.397]\n",
      "Epoch 019/100: 100%|██████████| 242/242 [00:00<00:00, 370.24it/s, optimizer=>RMSprop, loss=0.402]\n",
      "Epoch 020/100: 100%|██████████| 242/242 [00:00<00:00, 343.94it/s, optimizer=>RMSprop, loss=0.398]\n",
      "Epoch 021/100: 100%|██████████| 242/242 [00:00<00:00, 319.98it/s, optimizer=>RMSprop, loss=0.405]\n",
      "Epoch 022/100: 100%|██████████| 242/242 [00:00<00:00, 313.55it/s, optimizer=>RMSprop, loss=0.400]\n",
      "Epoch 023/100: 100%|██████████| 242/242 [00:00<00:00, 299.41it/s, optimizer=>RMSprop, loss=0.406]\n",
      "Epoch 024/100: 100%|██████████| 242/242 [00:00<00:00, 395.33it/s, optimizer=>RMSprop, loss=0.405]\n",
      "Epoch 025/100: 100%|██████████| 242/242 [00:00<00:00, 378.33it/s, optimizer=>RMSprop, loss=0.397]\n",
      "Epoch 026/100: 100%|██████████| 242/242 [00:00<00:00, 383.98it/s, optimizer=>RMSprop, loss=0.407]\n",
      "Epoch 027/100: 100%|██████████| 242/242 [00:00<00:00, 349.91it/s, optimizer=>RMSprop, loss=0.399]\n",
      "Epoch 028/100: 100%|██████████| 242/242 [00:00<00:00, 383.86it/s, optimizer=>RMSprop, loss=0.398]\n",
      "Epoch 029/100: 100%|██████████| 242/242 [00:00<00:00, 384.33it/s, optimizer=>RMSprop, loss=0.404]\n",
      "Epoch 030/100: 100%|██████████| 242/242 [00:00<00:00, 348.52it/s, optimizer=>RMSprop, loss=0.407]\n",
      "Epoch 031/100: 100%|██████████| 242/242 [00:00<00:00, 328.09it/s, optimizer=>RMSprop, loss=0.402]\n",
      "Epoch 032/100: 100%|██████████| 242/242 [00:00<00:00, 326.32it/s, optimizer=>RMSprop, loss=0.397]\n",
      "Epoch 033/100: 100%|██████████| 242/242 [00:00<00:00, 350.01it/s, optimizer=>RMSprop, loss=0.399]\n",
      "Epoch 034/100: 100%|██████████| 242/242 [00:00<00:00, 282.83it/s, optimizer=>RMSprop, loss=0.400]\n",
      "Epoch 035/100: 100%|██████████| 242/242 [00:00<00:00, 349.88it/s, optimizer=>RMSprop, loss=0.393]\n",
      "Epoch 036/100: 100%|██████████| 242/242 [00:00<00:00, 350.14it/s, optimizer=>RMSprop, loss=0.397]\n",
      "Epoch 037/100: 100%|██████████| 242/242 [00:00<00:00, 313.53it/s, optimizer=>RMSprop, loss=0.388]\n",
      "Epoch 038/100: 100%|██████████| 242/242 [00:00<00:00, 350.78it/s, optimizer=>RMSprop, loss=0.396]\n",
      "Epoch 039/100: 100%|██████████| 242/242 [00:00<00:00, 352.10it/s, optimizer=>RMSprop, loss=0.394]\n",
      "Epoch 040/100: 100%|██████████| 242/242 [00:00<00:00, 297.83it/s, optimizer=>RMSprop, loss=0.386]\n",
      "Epoch 041/100: 100%|██████████| 242/242 [00:00<00:00, 351.09it/s, optimizer=>RMSprop, loss=0.403]\n",
      "Epoch 042/100: 100%|██████████| 242/242 [00:00<00:00, 349.25it/s, optimizer=>RMSprop, loss=0.392]\n",
      "Epoch 043/100: 100%|██████████| 242/242 [00:00<00:00, 317.99it/s, optimizer=>RMSprop, loss=0.388]\n",
      "Epoch 044/100: 100%|██████████| 242/242 [00:00<00:00, 347.16it/s, optimizer=>RMSprop, loss=0.398]\n",
      "Epoch 045/100: 100%|██████████| 242/242 [00:00<00:00, 346.06it/s, optimizer=>RMSprop, loss=0.392]\n",
      "Epoch 046/100: 100%|██████████| 242/242 [00:00<00:00, 302.29it/s, optimizer=>RMSprop, loss=0.400]\n",
      "Epoch 047/100: 100%|██████████| 242/242 [00:00<00:00, 339.76it/s, optimizer=>RMSprop, loss=0.394]\n",
      "Epoch 048/100: 100%|██████████| 242/242 [00:00<00:00, 351.93it/s, optimizer=>RMSprop, loss=0.392]\n",
      "Epoch 049/100: 100%|██████████| 242/242 [00:00<00:00, 335.99it/s, optimizer=>RMSprop, loss=0.393]\n",
      "Epoch 050/100: 100%|██████████| 242/242 [00:00<00:00, 336.41it/s, optimizer=>RMSprop, loss=0.399]\n",
      "Epoch 051/100: 100%|██████████| 242/242 [00:00<00:00, 310.83it/s, optimizer=>RMSprop, loss=0.397]\n",
      "Epoch 052/100: 100%|██████████| 242/242 [00:00<00:00, 311.34it/s, optimizer=>RMSprop, loss=0.398]\n",
      "Epoch 053/100: 100%|██████████| 242/242 [00:00<00:00, 321.57it/s, optimizer=>RMSprop, loss=0.389]\n",
      "Epoch 054/100: 100%|██████████| 242/242 [00:00<00:00, 346.86it/s, optimizer=>RMSprop, loss=0.397]\n",
      "Epoch 055/100: 100%|██████████| 242/242 [00:00<00:00, 352.93it/s, optimizer=>RMSprop, loss=0.390]\n",
      "Epoch 056/100: 100%|██████████| 242/242 [00:00<00:00, 317.98it/s, optimizer=>RMSprop, loss=0.387]\n",
      "Epoch 057/100: 100%|██████████| 242/242 [00:00<00:00, 351.86it/s, optimizer=>RMSprop, loss=0.398]\n",
      "Epoch 058/100: 100%|██████████| 242/242 [00:00<00:00, 327.94it/s, optimizer=>RMSprop, loss=0.390]\n",
      "Epoch 059/100: 100%|██████████| 242/242 [00:00<00:00, 318.76it/s, optimizer=>RMSprop, loss=0.385]\n",
      "Epoch 060/100: 100%|██████████| 242/242 [00:00<00:00, 345.89it/s, optimizer=>RMSprop, loss=0.391]\n",
      "Epoch 061/100: 100%|██████████| 242/242 [00:00<00:00, 349.08it/s, optimizer=>RMSprop, loss=0.387]\n",
      "Epoch 062/100: 100%|██████████| 242/242 [00:00<00:00, 320.38it/s, optimizer=>RMSprop, loss=0.394]\n",
      "Epoch 063/100: 100%|██████████| 242/242 [00:00<00:00, 331.70it/s, optimizer=>RMSprop, loss=0.388]\n",
      "Epoch 064/100: 100%|██████████| 242/242 [00:00<00:00, 347.59it/s, optimizer=>RMSprop, loss=0.383]\n",
      "Epoch 065/100: 100%|██████████| 242/242 [00:00<00:00, 317.88it/s, optimizer=>RMSprop, loss=0.389]\n",
      "Epoch 066/100: 100%|██████████| 242/242 [00:00<00:00, 347.89it/s, optimizer=>RMSprop, loss=0.386]\n",
      "Epoch 067/100: 100%|██████████| 242/242 [00:00<00:00, 346.85it/s, optimizer=>RMSprop, loss=0.387]\n",
      "Epoch 068/100: 100%|██████████| 242/242 [00:00<00:00, 313.60it/s, optimizer=>RMSprop, loss=0.384]\n",
      "Epoch 069/100: 100%|██████████| 242/242 [00:00<00:00, 349.80it/s, optimizer=>RMSprop, loss=0.395]\n",
      "Epoch 070/100: 100%|██████████| 242/242 [00:00<00:00, 330.00it/s, optimizer=>RMSprop, loss=0.391]\n",
      "Epoch 071/100: 100%|██████████| 242/242 [00:00<00:00, 317.35it/s, optimizer=>RMSprop, loss=0.382]\n",
      "Epoch 072/100: 100%|██████████| 242/242 [00:00<00:00, 346.21it/s, optimizer=>RMSprop, loss=0.386]\n",
      "Epoch 073/100: 100%|██████████| 242/242 [00:00<00:00, 306.65it/s, optimizer=>RMSprop, loss=0.389]\n",
      "Epoch 074/100: 100%|██████████| 242/242 [00:00<00:00, 317.47it/s, optimizer=>RMSprop, loss=0.386]\n",
      "Epoch 075/100: 100%|██████████| 242/242 [00:00<00:00, 345.91it/s, optimizer=>RMSprop, loss=0.376]\n",
      "Epoch 076/100: 100%|██████████| 242/242 [00:00<00:00, 323.23it/s, optimizer=>RMSprop, loss=0.389]\n",
      "Epoch 077/100: 100%|██████████| 242/242 [00:00<00:00, 310.91it/s, optimizer=>RMSprop, loss=0.390]\n",
      "Epoch 078/100: 100%|██████████| 242/242 [00:00<00:00, 345.41it/s, optimizer=>RMSprop, loss=0.386]\n",
      "Epoch 079/100: 100%|██████████| 242/242 [00:00<00:00, 353.76it/s, optimizer=>RMSprop, loss=0.382]\n",
      "Epoch 080/100: 100%|██████████| 242/242 [00:00<00:00, 317.48it/s, optimizer=>RMSprop, loss=0.389]\n",
      "Epoch 081/100: 100%|██████████| 242/242 [00:00<00:00, 287.64it/s, optimizer=>RMSprop, loss=0.388]\n",
      "Epoch 082/100: 100%|██████████| 242/242 [00:00<00:00, 308.45it/s, optimizer=>RMSprop, loss=0.387]\n",
      "Epoch 083/100: 100%|██████████| 242/242 [00:00<00:00, 336.46it/s, optimizer=>RMSprop, loss=0.395]\n",
      "Epoch 084/100: 100%|██████████| 242/242 [00:00<00:00, 296.80it/s, optimizer=>RMSprop, loss=0.387]\n",
      "Epoch 085/100: 100%|██████████| 242/242 [00:00<00:00, 327.21it/s, optimizer=>RMSprop, loss=0.381]\n",
      "Epoch 086/100: 100%|██████████| 242/242 [00:00<00:00, 321.70it/s, optimizer=>RMSprop, loss=0.393]\n",
      "Epoch 087/100: 100%|██████████| 242/242 [00:00<00:00, 285.20it/s, optimizer=>RMSprop, loss=0.386]\n",
      "Epoch 088/100: 100%|██████████| 242/242 [00:00<00:00, 330.24it/s, optimizer=>RMSprop, loss=0.389]\n",
      "Epoch 089/100: 100%|██████████| 242/242 [00:00<00:00, 319.13it/s, optimizer=>RMSprop, loss=0.386]\n",
      "Epoch 090/100: 100%|██████████| 242/242 [00:00<00:00, 321.67it/s, optimizer=>RMSprop, loss=0.388]\n",
      "Epoch 091/100: 100%|██████████| 242/242 [00:00<00:00, 291.73it/s, optimizer=>RMSprop, loss=0.390]\n",
      "Epoch 092/100: 100%|██████████| 242/242 [00:00<00:00, 292.05it/s, optimizer=>RMSprop, loss=0.387]\n",
      "Epoch 093/100: 100%|██████████| 242/242 [00:00<00:00, 314.56it/s, optimizer=>RMSprop, loss=0.390]\n",
      "Epoch 094/100: 100%|██████████| 242/242 [00:00<00:00, 282.92it/s, optimizer=>RMSprop, loss=0.388]\n",
      "Epoch 095/100: 100%|██████████| 242/242 [00:00<00:00, 320.00it/s, optimizer=>RMSprop, loss=0.377]\n",
      "Epoch 096/100: 100%|██████████| 242/242 [00:00<00:00, 313.24it/s, optimizer=>RMSprop, loss=0.385]\n",
      "Epoch 097/100: 100%|██████████| 242/242 [00:00<00:00, 313.53it/s, optimizer=>RMSprop, loss=0.381]\n",
      "Epoch 098/100: 100%|██████████| 242/242 [00:00<00:00, 293.57it/s, optimizer=>RMSprop, loss=0.379]\n",
      "Epoch 099/100: 100%|██████████| 242/242 [00:00<00:00, 315.61it/s, optimizer=>RMSprop, loss=0.399]\n",
      "Epoch 100/100: 100%|██████████| 242/242 [00:00<00:00, 312.15it/s, optimizer=>RMSprop, loss=0.390]\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "log_interval = 20\n",
    "result_research = dict()\n",
    "form = len(str(epochs))\n",
    "criterion = MSELoss()\n",
    "for optimizer_func in optimizer_list:\n",
    "    model = great_california_model(8,1).cuda()\n",
    "    if optimizer_func == torch.optim.SGD:\n",
    "        optimizer = optimizer_func(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    else:\n",
    "        optimizer = optimizer_func(model.parameters())    \n",
    "    optimizer_name = optimizer.__class__.__name__\n",
    "    print(optimizer_name)\n",
    "    numder_of_batchs = len(train_loader)\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        model.train()\n",
    "        with trange(numder_of_batchs, desc=f'Epoch {(epoch + 1):0{form}}/{epochs}') as t:\n",
    "            # t.set_postfix_str(f'optimizer=>{optimizer_name}')\n",
    "            for batch_idx, (sample, target) in zip(t, train_loader):\n",
    "                \n",
    "                sample = sample.cuda()\n",
    "                target = target.cuda()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                output = model(sample)\n",
    "                loss = criterion(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                if batch_idx % log_interval == 0 and not batch_idx:\n",
    "                    t.set_postfix_str(f'optimizer=>{optimizer_name}, loss={loss.item():.3f}')\n",
    "                if batch_idx == numder_of_batchs - 1:\n",
    "                    t.set_postfix_str(f'optimizer=>{optimizer_name}, loss={(running_loss / numder_of_batchs):.3f}')\n",
    "        if epoch in [9, 49, 99]:           \n",
    "            model.eval()\n",
    "            epo = epoch + 1\n",
    "            if epo not in result_research:\n",
    "                result_research[epo] = dict()\n",
    "            with torch.no_grad():\n",
    "                output = model(x_test.cuda())\n",
    "                loss_out = torch.sqrt(criterion(output, y_test.cuda()))\n",
    "                # print(f'Лосс на модели {loss_out.item()}')\n",
    "                result_research[epo][optimizer_name] = loss_out.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результаты теста:\n",
      "На кофигурации сети:\n",
      "Sequential(\n",
      "  (0): Linear(in_features=8, out_features=64, bias=True)\n",
      "  (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU()\n",
      "  (3): Dropout(p=0.25, inplace=False)\n",
      "  (4): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (5): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (6): ReLU()\n",
      "  (7): Dropout(p=0.25, inplace=False)\n",
      "  (8): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n",
      "На 10 эпохах: \n",
      "SGD => Loss: 0.679122269153595\n",
      "Adam => Loss: 0.9135489463806152\n",
      "RMSprop => Loss: 1.226797103881836\n",
      "Лучший результат показала функция оптимизации ['SGD']\n",
      "\n",
      "На 50 эпохах: \n",
      "SGD => Loss: 1.2081718444824219\n",
      "Adam => Loss: 0.6837319731712341\n",
      "RMSprop => Loss: 2.532555103302002\n",
      "Лучший результат показала функция оптимизации ['Adam']\n",
      "\n",
      "На 100 эпохах: \n",
      "SGD => Loss: 0.6326267719268799\n",
      "Adam => Loss: 1.1722090244293213\n",
      "RMSprop => Loss: 2.381446599960327\n",
      "Лучший результат показала функция оптимизации ['SGD']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Результаты теста:')\n",
    "print(f'На кофигурации сети:\\n{model}')\n",
    "for j in result_research.keys():\n",
    "    print(f'На {j} эпохах: ')\n",
    "    for i in result_research[j].keys():\n",
    "        print(f'{i} => Loss: {result_research[j][i]}')\n",
    "    winner = [key for key, value in result_research[j].items() if value == min(result_research[j].values())]   \n",
    "    print(f'Лучший результат показала функция оптимизации {winner}\\n')\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
